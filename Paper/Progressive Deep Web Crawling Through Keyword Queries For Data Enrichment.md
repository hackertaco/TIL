## 12/18 
- 수학적 증명에 대한 이해가 부족하여, 나머지 4쪽은 읽지 않음. 더 공부하고 다시 읽고 싶은 논문.

외부 데이터 소스에서의 새로운 속성을 로컬 데이터베이스에서 확장하는 개념이 Data enrichment
이 논문에서는, 어떻게 hidden database를 키워드 서치 API를 통해 점진적으로 크롤링을 할지에 대하여 학습한다.

이것은 도전을 요한다. 왜냐하면 데이터 액세스를 보통 제한하기 때문이다. 

이를 해결하기위해, SmartCrawl을 제안한다. 

쿼리 예산 b가 주어졌을 때, 로컬 데이터베이스에 기반한 쿼리 풀을 먼저 설계하고, 로컬 데이터베이스를 커버할 수 있는 가장 최대치의 쿼리 결과를 보여줄 쿼리를 hidden database에 등록한다.

주요 기술적 과제는 쿼리가 주어졌을 때 커버되는 로컬 데이터 수를 어떻게 측정하느냐다.

가장 간단한 접은은 로컬 데이터베이스에서의 쿼리 빈도를 측정하는 것이지만, 그는 효과적이지 않다.
그 이유는 델타값의 영향 때문이다. hidden database에서 찾아보지 못하는 로컬 데이터의 수를 델타라고 한다.
그리고, 두번째는 hidden database에 의해 강요되는 제한들 때문이다.

우리는 어떻게 이 두가지 문제를 완화할지를 연구하고, 퍼포먼스를 향상시킬 기술들을 제안한다.

실험적인 결과는 시뮬레이팅과 실세계 hidden database 모두를 보여준다. SmartCrawl이 확연하게 커버리지를 늘린다.

## Introduction
로우 데이터를 파싱하는 과정에 데이터 사이언스트들이 시간을 80프로는 사용한다. 
그건 데이터 모으고, 데이터 클리닝, data enrichment를 말하는데 우리는 data enrichment에 집중한다.
그는 추출한 외부 데이터 베이스의 새로운 특징을 가지고 로컬 데이터베이스를 확장하는 것이다.
(예시: 새로이 오픈한 레스토랑 리스트를 가지고 왓는데 그걸 가지고 카테고리와 점수를 알고 싶어하는 것)

data enrichment는 새로운 인사이트를 발견하게 한다. Enriched data는 에러 발견에도 사용된다.
이는 새로운 연구 주제는 아니라서, 이미 웹 테이블이라는 개념이 있다. 그러나, 데이터 사이언티스트들은 종종 그 웹테이블 코퍼스가 없다. 
보통 제한된 쿼리 인터페이스를 가진 데이터베이스를 사용하는데, 그를 deep web이라 부른다.

그렇기 때문에 빨리 로컬 데이터를 enrich하고 싶은 데이터 사이언티스트들은 제한이 걸린다. 
그래서, 현명하게 쿼리세트를 고르는 것이 중요하다. 유저의 로컬 데이터베이스를 가장 많이 담을 수 있게끔 하는 것이다.

이 문제를 우리는 CrawlEnrich라고 부른다. 쿼리 결과가 한 쿼리를 했는데 로컬 레코드와 똑같은 엔티티를 언급하는 히든 레코드를 포함하고 있다고 가정하자. 

로컬 데이터 베이스 -> D, 히든 데이터베이스 -> H, 정해진 예산 -> b
CrawlEnrich의 목표는 b쿼리의 결과가 가능한 한 많은 데이터를 담을 수 있게 하는 것이다.

우리는 간단한 변수라도 굉장히 도전적이라는 것을 알아냈다. NaiveCrawl이라고 명명한, 로컬 데이터베이스의 각 레코드를 만드는 서치쿼리가 있다.
이는 로컬데이터베이스가 커질수록 쿼리 수도 늘고, 쿼리들은 데이터 에러에 취약하다.
이 경우 쿼리들은 과도하게 정확하면 안되고, 또 과하게 느슨해도 안된다. 

근본적인 문제는 쿼리-이득 예측이다: 우리는 검색 쿼리가 주어지면 어떻게 예상 수를 예측할까?
이 예상 숫자는 쿼리 베네핏이라고 불린다. 
일단 쿼리 베네핏이 예상되면, 문제는 줄어든다.

이를 위해, 우리는 SmartCrawl을 개발했다. 이는 쿼리 풀을 일단 기존 데이터베이스에서 건설하고, 어떻게 하면 가장 많은 결과를 낼 수 있을지에 대해서 쿼리를 고른다.
QSel-Simple로 간단하게 접근한다. 이 접근은 쿼리 베네핏 측정을 위해 기존 데이터베이스와 관련한 쿼리 빈도를 사용한다.
이는 그 쿼리를 포함하는 레코드 수로 정의된다.
이를 이상적인 접근과 비교해본다. (정확히 맞는 쿼리 개수랑)
그리고 어떤 차이가 있는지 비교한다. 그래서 QSel-Simple 에 영향을 줄 두가지 요인을 발견하고는, 몇몇 가정 내에서는 이상적인 결과와 똑같다는 걸 알아냈다.

- 요인 1: 델타 임팩트

D가 H에 완전하게 커버되는지 아닌지에 대한 개념. 만약 아니라면, D가 H에서 찾아지지 않는 세트를  ∆D = D −H로 표현한다.
예를 들자면, D에는 전부 있고 H에는 그 쿼리를 찾아볼 수 없을 때, 그 쿼리에는 아무런 이득이 없다.
그러므로, 우리는 D − ∆D를 쿼리 이득으로 예측해야한다.
그 이유로, 우리는 |∆D의 안좋은 영향을 줄이기 위한 기술을 제안한다.

- 요인 2: Top-k 제한
  top-k constraint에 의해 영향을 받는지 아닌지에 대한 요인.
H에서 k 레코드들을 더 많이 맞추게 된다면 그는 k 제한에 영향을 받는 쿼리라고 부른다.
이를 예측하는 QSel-Est 접근을 만든다. 
이를 통해 7배는 더 에러에 강하게 만들엇다.

요약하자면, 이 논문을 통해서
1. 알고있는 한 처음으로 crawlEnrich 문제를 연구했다. 그래서 SmartCrawl을 제안했다.
2. QSel-Simple이라는 간단한 쿼리 선택 전략을   만들었다. 그리고 델타와 top k 강제 요인을 명확화햇다.
3. 델타가 어떻게 나쁜 영향을 QSel-Simple에 미치는지 분석하고 효과적인 방법을 고안했다.
4. 우리는 어떻게 top-k 강제를 부수고 참신한 예측기를 만들지를 연구했다.
5. 실제 히든 데이터베잇ㅅ스와 시뮬레이팅해본 데이터베이스를 넘어 확장적 실험을 수행한다. 

# 문제 구체화
로컬데이터베이스와 히든 데이터베이스를 우선 모델링했다.
키워드들로 구성된 세트를 포함하는 키워드 쿼리를 q라고 명명한다. 
이 논문은 크롤링 부분에 집중한다. 
완전한 엔드투엔드 데이터 enrichment 시스템은 스키마 매칭같은 추가 기능이 필요하다. 즉, 로컬 D와 H 사이의 스키마를 맞추는 것이다.
그러나 그는 직교 이슈로서 다뤄질 수도 있다. 우리는 사전 논문을 통해서 존재하는 스키마 매칭과 엔티티 결정을 하는 기술을 제안했다. 
그래서 스키마는 얼라인되어있고, 우리는 블랙박스로서 엔티티 결정을 다룬다는 것을 가정한다. 


- 문제인식
match(d, h)라는 함수를 정의한다. 
D ∩ H = {d ∈ D | h ∈ H, match(d,h) = True}
만약 d와 h가 같은 실세계 엔티티를 언급하게되면 True다. 

1. D의 커버리지를 최대화하기 위하여, Qsel 세트를 고르는 것이 목표다. 
불행하게도, CrawlEnrich는 NP-hard한 문제라, 맥시멈 커버리지 문제로부터 감소가 있어야 증명된다. 
사실, 예외적으로 이 문제를 힘들게하는 것은 맥시멈 커버리지 문제를 푸는데 사용되는 그리디 알고리즘이 적용이 어렵기 때문이다.
이 논문에서는, 키워드 서치 인터페이스를 고려해본다.

2. 키워드 서치 인터페이스
사용자에게 쿼리를 받아 top-k 레코드를 return 한다.
여기서 쿼리가 오버플로우 하거나 solid하다: 오버플로우의 경우 한 쿼리 결과가 k개 이상 나오는 경우, solid는 그 반대인 경우

키워드 매칭 규칙과 관련하여, 대부분의 웹사이트들은 합쳐진 키워드 서치 인터페이스를 가지고 있다는 걸 파악했다.
즉, 모든 쿼리 키워드를 포함하는 레코드만 리턴한다.
따라서, 우리 또한 합쳐진 키워드 서치 인터페이스로 가정한다.

top-k 강제 때문에, 서치 인터페이스는 리턴된 레코드 수를 제한해야한다. 따라서 쿼리 frequency가 k 보다 크면, unknown 랭킹 함수에 의해서 레코드를 처리하고, top-k개의 레코드를 리턴한다.
솔리드 쿼리를 좀 더 정확하게 본다. 

# SmartCrawl Framework
우선 쿼리 풀을 D로부터 만들고, 가장 효과 좋은 쿼리를 골라낸다.

- 쿼리 풀 형성
  Q를 쿼리 풀이라고 정의하고, 만약 쿼리 q가 어떤 로컬 자료에도 보이지 않을 때 |q(D)| = 0이다.

  - 쿼리풀이 모든 로컬 레코드를 보기, 둘째는 우리는 쿼리풀이 다양한 로컬 레코드를 커버할 쿼리를 갖기를 바란다.
    - 첫 원리를 만족하기 위해, 각 로컬 레코드에서 특정한 쿼리를 형성한다.
    - 둘째 원리를 만족하기위해, Frequent Pattern Mining 알고리즘을 사용해서 효과적으로 쿼리들을 만들어낸다.
    - 두 원리들로 쿼리 풀을 만들어낼 것이다.
    - 쿼리가 만들어지는 데 쓰이는 시간들과, 만들어진 쿼리 수 사이의 tradeoff 균형을 맞추는 t
    - 실험에서, t 밸류를 설정하여 frequent pattern mining algorithm이 예산의 5배정도의 쿼리를 만들어내게끔한다.
  

- 쿼리 셀렉션
  우선 쿼리 풀이 만들어지면, 쿼리를 고른다. (QSel-Ideal, QSel-Simple, and QSel-Est)
  - QSel-Ideal은 사전에 쿼리의 효과를 안다고 가졍했을 때, 그 중에서도 가장 효과있는 쿼리를 선정해서 반복한다.
    - 그러나, 닭과 달걀 딜레마에 빠진다. 쿼리가 등록될때까지는 실제로 효과를 보기가 힘들다. 그런데 효과를 알아야만한다. 
    - 간단하게는 쿼리 빈도를 알아야한다. 
  - QSel-Simple은 쿼리 빈도를 사용한다는 점에서 IDEAL과 다르다. 
    - 이는 ∆D와 top-k 강제의 영향에 의해 퍼포먼스가 영향을 받는다. 
    - D는 H에 의해 완전히 커버된다는 가정, H는 top-k 가정을 하지 않는다는 가정. 

# |∆D|의 영향
|∆D|이 QSel-Simple and QSel-Ideal 사이 어떻게 퍼포먼스 갭에 영향을 주는지 알아볼 것이다.

QSel-Ideal이 QSel-Simple보다 훨씬 큰 수를 커버하는가? 
- 이해해보기
  - QSel-Bound라는 대리 알고리즘을 생각해본다. 우리는 먼저 QSel-Ideal and QSel-Bound 사이의 갭을 비교하고, QSel-Bound
    and QSel-Simple을 비교한다.
  - QSel-Bound는 잘못 선택된 쿼리를 원하지 않아서, 그걸 발견하면 쿼리 풀에 집어넣는다. 
  - 쿼리가 히든 데이터베이스로부터 일치하는 쿼리를 발견해왔을 때, 기존 로컬과 다르다면, 그 둘 사이 결과값의 차이를 로컬 데이터 베이스에서 제한것을 로컬 데이터 베이스라 명명한다.
  - QSel-Bound는 그런데 이미 선택되었던 쿼리를 보존하기 때문에, 예산을 낭비한다. 따라서 그냥 QSel-Simple을 택한다.
  
- |∆D|의 안 좋은 영향 완화하기
  - |∆D|값이 크면 정확하지 않으므로, 이를 완화해야한다.
  - |∆D|를 찾아 D에서 제거하고자한다. H에 없는 쿼리를 찾아서 그걸 가진 데이터를 삭제한다.

# Top-K 강제성
- 쿼리 타입 예측
q쿼리가 주어졌을 때 그것이 solid or overflowing인지 예측하고자 한다. 샘플링H와 샘플링 비율을 계산하여 k값과 비교한다.
- Solid쿼리 예측기
  - 우선 unbiased 예측기를 제안한다. benefit(q) = |q(D) ∩ q(H)k|. solid일 경우, q(H )k = q(H )이기 때문에, benefit(q) = |q(D) ∩ q(H )|.
  - 이는 잘 동작하지는 않는다. 왜냐하면 q(D)가 너무 작기 때문이다. 보통 그래서 베네핏이 0이다. 별로 쿼리 선택에 유용하지는 않다.
  - 두번째는 biased 예측기다. ∆D값은 보통 굉장히 작다. 크더라도, 그 사이즈를 줄일 수 있다. 그런 이유로, benefit(q) = |q(D) − q(∆D)| = |q(D)| − |q(∆D)|.에서 benefit(q) ≈ |q(D)|,로 추론할 수가 있다.
  - 이 biased가 더 정확하다는 것을 실험을 통하여 발견하였다.
- Overflowing 쿼리 예측기
  - overflow 쿼리는 세 가지 요소에 영향을 받는다. : |q(D)|, |q(H )|, k
  - 예측기를 만들기 위해, 이걸 어떻게 시스템화할지 고민했다.
  1. 기본 생각
  - 초기하 분포를 기반으로, 
    E[benefit(q)] = n · k/ N = |q(D) ∩ q(H )| · k |q(H)|를 유추해볼 수 있다.
  - 그런데, 만약 q(D) ∩ q(H )이 biased 샘플이라면 비율에 맞추어 각각 조정해주어야한다.
  - 하지만 로컬 테이블이 유저에 의해 제공되므로, 유저가 그 가중치를 특정하기가 쉽지않다. 

  2. 예측기
  - |q(D) ∩ q(H )|과 q(H)이 알려지지않아서 위의 식을 사용하긴 어렵다. 
  - 따라서 샘플 Hs를 이용하여 예측해보려한다. θ 도입

  3. QSel-Est 도입
  - 최적화 기술을 이용해, QSel-Simple를 보완한 것이 QSel-Est다.
  - 쿼리 선택 과정이 비슷하다. 

# 실험
확장적 실험 질행. 다섯개의 주요 질문들을 실험했다. 
1) SmartCrawl이 NaiveCrawl보다 얼마나 잘 작동하는지?
2) QSel-Est가  QSel-Simple에 비하여 얼마나 잘 작동하는지?
3) 어떤 예측기가 더 좋은지?
4) SmartCrawl이 NaiveCrawl보다 데이터에 더 강한지?
5) SmartCrawl이 conjunctive한 키워드 서치 인터페이스보다 더 잘 작동하는지?

## 실험 세팅
1. H 시뮬레이팅
- 데이터 사이언티스트가 몇몇 도메인에서 논문을 모으는데  각각의 URL을 알고싶어한다고 가정한다. 
  - D와 H 관련 세팅: H에 의해 D가 완전히 커버되는 것이 아니므로, H는 두가지를 가지고 있다고 가정한다. H − D 의 경우 랜덤하게 데이터셋을 모았을 때, H ∩ D는 D에서 랜덤하게 뽑아져나왔을 때
  - 키워드 서치 인터페이스: 서치 엔진을 실행했다. 3개 이상의 속성이 쿼리에 주어지면 모든 키워드를 포함한 상태로 줄세우기를 한다.
  - 평가 메트릭스: 각 접근의 퍼포먼스 체크를 위해 커버리지를 사용한다. 로컬 레코드 전체 수로 정의된다. 상대적 커버리지는 로컬 레코드에서의 퍼센트로 정의된다. 
  - 파라미터: 여러 인수들을 설정. 에러 퍼센트도 설정

2. 실제 H
SmartCrawl이 실제 H와 비교해 얼마나 성과를 내는지 비교
- D: Yelp 데이터셋을 기반으로 D를 만들었다. 
- H: Yelp과 Spotify 사용
- H샘플: 샘플링 비율에 따라서 샘플을 만들어내는 기술을 도입했다. 쿼리 풀로 사용할 싱글 키워드를 추출했다. 
- 평가 메트릭: 수동으로 데이터라벨링을 했다. 일단 크롤링되면, 엔티티 resolution 컴포넌트는 완벽히 맞는 로컬 레코드를 찾을 수 있다. 

3. 서로 다른 접근을 실행하기
- Naive-Crawl: 여러 속성과 연결되어있다. 
- HiddenCrawl: H를 크롤링하는 목적이있다. H샘플이 HiddenCrawl에서 사용가능하게 해두었다.
- SmartCrawl-S, SmartCrawl-U, SmartCrawl-B.: SmartCrawl의 여러 변수를 고려한다. 쿼리 선택에 있어서 다르다. 
- IdealCrawl

## 시뮬레이션 실험

SmartCrawl의 퍼포먼스를 평가했다. 여러 상황에서 비교도했다.

1. 샘플링 비율
- 샘플링 비율에 따른 커버리지 차이를 비교. 샘플이 적어도 SmartCrawl-B는 잘 작동했다.
-  SmartCrawlU는 좋지않은 퍼포먼스를 내었다.

2. 로컬 데이터베이스 사이즈
- D가 상대적으로 커서 HiddenCrawl가 좋은 성적을 냈는데, 작으면 잘 안된다.
- NaiveCrawl은 그 사이즈와는 관계없다.

3. 결과 수 제한
- K값에 따라서 달라지는 결과, 커질수록, top-k 영향력은 줄어든다.

4. ∆D임팩트
- ∆D가 커지면 모든 접근에서 퍼포먼스가 떨어진다. 그럼에도, SmartCrawl-B는 잘해낸다.

5. 데이터 에러에 강한지
   NaiveCrawl이 주로 더 많은 키워드를 가지고 있어서, 데이터에러에 영향을 받는다.

## 실세계 H
SmartCrawl이 얼마나 가정 없이 퍼폼할 수 있을지가 궁금했다. 

SmartCrawl-B, NaiveCrawl, and HiddenCrawl의 퍼포먼스 비교했다.
SmartCrawl-B 는 80%리콜했다. 


# 결론 
CrawlEnrich라는 참신한 문제를 다루었다. SmartCrawl 프레임워크를 제안해, 로컬 데이터베이스 커버리지를 최대화할 쿼리세트를 선택하게했다.
문제는 어떻게 가장 좋은 쿼리를 찾는가였다. QSel-Est은 biased 예측기를 사용하여 더 좋은 퍼포먼스를 내었다.
SmartCrawl은 다른 크롤러보다 월등한 퍼포먼스를 내었고, 데이터 에러에도 강하였다.

