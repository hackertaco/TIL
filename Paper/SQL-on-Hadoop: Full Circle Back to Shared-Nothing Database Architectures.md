# 12/29

하둡 데이터로 SQL 쿼리 프로세싱을 하는 것은 최근 인기가 있다. 
하둡에 여러 sql 서포팅을 하는 시스템 중, 하이브는 맵 리듀스와 같은 sql-like한 선언을 하는 첫 네이티브 하둡 시스템이다.

반면에 임팔라는, 하둡 위에서 병렬 데이터베이스 아키텍처를 이룩하는 새로운 sql-on-하둡 시스템이다.

둘 모두 컬럼 지향 스토리지를 통한 데이터 처리를 극대화하고, 서로 다른 파일 포맷을 지원한다. 

이 연구에서는, 이 TPC-H와 두개의 TPC-DS 워크로드를 사용하여 두 시스템의 퍼포먼스를 비교한다. 
I/O 효율성도 면밀히 연구한다. 

우리의 결과는 임팔라가 맵리듀스가 3-4배 정도 빠르고 Tez가 2.1에서 2.8배 빠르다는 것이었다.
또한, TPC-DS에서는 맵리듀스가 8.2에서 10배정도 빨랐다. 

이 이유를 알고자한다.

## Introduction
기업들은 중앙 데이터 레포지토리로 하둡을 사용한다. 이는 다양한 구조의 데이터를 관리하고 깊은 분석을 하기위해 사용된다.

그 중에 특히, SQL 프로세싱은 많은 기업의 데이터 관리 툴이 sql에 의존적이고, 그에 익숙하여 인기가 많다. 

우리는 이 하둡 시스템을 두개의 카테고리로 나눠볼 수 있다. 
1) 네이티브 하둡 기반 시스템
2) 데이터베이스와 하둡 하이브리드

첫번째 카테고리에서는, 하이브가 MapReduce와 같은 sql-like한 쿼리를 처리하는 첫 시스템이다. 
이 카테고리에서, 임팔라는 공유하지않는 병렬 데이터 베이스 아키텍처로 이동했다. 그는 그 자체의 데몬을 이용하고 중간 결과를 구체화하는 대신 파이프라이닝을 한다. 

두번째 카테고리에서는, Hadapt, Microsoft PolyBase 등이 있다. 전체적으로, 우리는 무공유 데이터베이스 아키텍처로의 수렴을 관찰했다.

이 논문에서는, 첫번째 카테고리에 집중하고, 하이브와 임팔라의 퍼보먼스를 조사한다. 
둘을 고른 것은 아키텍처가 서로 다를 뿐 아니라 둘이 인기가 많기 때문이다.
둘은 주요 하둡 분산 벤더들이 제공하는 SQL 오퍼링이다. 결과적으로, 많은 회사에서 사용한다. 
다른 것들은 회사 자체 서비스라서 널리 쓰이지는 않는다. 

하이브와 임팔라는 분석 쿼리에 사용된다. 컬럼 기반 데이터베이스는 최근 관계형 데이터베이스에서 좀 퍼포먼스를 올리는 쪽으로 보여지고잇다.
결과적으로, 많은 분석 쿼리에서 사용된다.
이 컬럼기반 데이터베이스 파일 포맷으로 여러개가 제시되었는데, 좀 퍼포먼스적으로 문제가 있었다. 
임팔라는 Parquet이라는 파일 포맷을 제시했다. 기존 것과 경합 중이다.

이 논문에서, 우리는 두개의 스토리지 포맷을 I/O 특징에서 인사이트를 얻기위해 면밀히 살핀다. 
임팔라와 하이브의 세세한 분석을 진행한다. 

### 임팔라의 데이터베이스-like 아키텍처는 큰 퍼포먼스 성과를 제공한다. 
- 하이브는 맵리듀스를 쓰는 데 있어서 스케줄링 오버헤드로 중간 결과를 쓰는 데 비용이 잇다.
- 임팔라는 반면에 쿼리 컴퓨팅 결과로 데이터를 스트림한다.

그러나, 임팔라는 중간 쿼리 실패 회복을 아직 못한다. 

- Tez는 오버헤드를 제거는 해주지만, deserialization 오버헤드를 제거하지 않아 임팔라보다 늦다.

### 임팔라의 I/O 부차 시스템은 훨씬 빠른 처리율을 보인다. 
### Parquet 포맷은 더 효율적으로 불필요한 데이터를 전처리한다. 

이 결과로, 우리는 무공유 데이터베이스 아키텍처가 MapReduce-base 런타임보다 분석 쿼리에 더 큰 이점이 있다는 것을 알게되었다. 

## 배경
### Hive 
Hive는 수천에서 수만개의 노드를 스케일링하는 능력이 있다. 

SQL-같은 쿼리 언어를 사용한다. 이는 Tez에서는 하나로, MapReduce에서는 여러개로 쪼개져 작업된다. 
Tez는 일시적 중간 파일을 만드는 대신, 실행 단계를 통해 데이터를 연결한다. 
이는 오버헤드를 줄인다.

조인 방식과 조인 순서는 퍼포먼스에 큰 영향을 끼친다. 
하이브는 두개의 조인 방식을 제공하는데, 리파티션 조인이라고 불린다. 여러 조인 쌍이 업무를 줄이기위해 섞인다. 
두번째는 map-side 조인이다. 리파티션 조인 단계를 줄이거나 셔플을 제거하는 것이다. 

### Impala
Hive와는 다르게, 다른 프레임워크에 기대기보다는 자체의 데몬을 제공한다. 
또한 무공유 병렬 데이터베이스 아키텍처를 가지고 있다. 
impalad라고 불리는 메인 데몬 프로세스는 쿼리 계획, 조정, 실행 엔진을 가지고 있다. 

임팔라는 두개의 조인 알고리즘을 가지고 있는데, 그는 파티션과 브로드 캐스트이다. 
각각은 하이브의 리파티션이나 맵사이드와 비슷하다.

임팔라는 아주 효율적인 I/O 레이어를 가지고 이싿. 
읽기 데이터 요청을과 실제 읽기 데이터를 분리한다. 

임팔라는 여전히 워킹 셋 쿼리를 요구하여 엄격한 제한을 둔다. 

### Columnar File Formats
Columnar 데이터 조직은 disk I/O를 줄이고 더 나은 압축과 분석 쿼리에 큰 혜택을 주는 인코딩 스킴을 가능하게 한다. 

하이브는 ORC, 임팔라는 Parquet 포맷을 이용한다.

#### ORC
이는 가벼운 색인 처리로 더 나은 I/O 효율뿐 아니라 데이터 인코딩과 블록레벨 압축으로 스토리지 효율성을 제공한다. 

또한 스트라이프 형태로 다양한 행 데이터를 저장한다.
![img.png](img/1229-1.png)

#### Parquet
효율적인 압축과 인코딩을 하기위하여 디자인되었다.

row group이라고 불리는 데이터 그룹을 만들고, 그는 컬럼 청크를 포함한다. 이는 다양한 페이지를 포함하고 디스크에 계속 쌓인다. 
페이지 레벨에서 이 압축 단계가 일어난다. 

메타데이타는 파일, 컬럼 청크, 페이지 계층 모두에 쌓인다. 
Parquet을 읽는 사람은 처음으로 어떤 쿼리가 처리돼야할지를 필터링하기위해 메타데이터를 파싱한다. 
이는 중첩 컬럼은 아직 제공하지 않는다. 

최대 1기가 바이트까지 제공한다.

![img_1.png](img/1229-2.png)

## 클러스터 실험
### 작업량으로서의 TPC-H
22 읽기 전용 TPC-H 쿼리를 포함하고 리프레시 스트림은 제공하지않는다. 

TPC-H를 1000기가까지 사용한다.

하이브 맵리듀스에서는 우리는 여러 다양한 최적화를 진행했다
1) 연관 쿼리 최적화
2) 푸시다운 예측과 인덱스 필터링
3) 맵사이드 조인과 집계

여러개의 쿼리로 쿼리가 나누어질 때, 중간 결과는 일시적 테이블로 구체화된다. 하이브는 전형적으로 큰 수의 리듀스 업무를 생성하는데, 이는 나쁜 퍼포먼스를 일으킨다.
우리는 우리의 환경 속에서 최적의 퍼포먼스를 낸다는 것을 파악했다.

임팔라와 관련해서는, 테이블과 컬럼 통계를 진행했다. 
이 통계는 조인 순서와 방법을 최적화하는 데 사용됐다.


### 데이터 준비와 로드 시간
- ORC, Parquet, TXT 파일 포맷을 모두 준비했다.
- 압축 알고리즘으로 snappy, gzip, zlib 사용하는데 스내피가 성능이 좋다. 따라서, 우리는 그저 스내피 결과만 보고할 것이다.
- 우선 데이터 파일을 TPC-H생성기로 1테라 정도의 스케일로 생성, HDFS에 복사한다. 각각의 파일 포맷으로 변환한다.

### 전반적 퍼포먼스
22 TPC-H 쿼리를 진행해보았을 대, 하이브와 임팔라에서 실행 시간을 계산했다. 
일단 모든 파일 캐시를 삭제했고, 세개의 풀 런을 진행했다.
각 파일 포맷별로 세번을 진행했고, 평균 반응 시간을 쟀다.

임팔라가 기사, 산술 평균에서 압도적으로 좋은 성과를 내었다.
Parquet보다 txt 형식일 때, 압도적으로 더 빠르지는 않았다. 게다가, snappy 압축은 임팔라의 퍼포먼스를 더 향상시키지는 않았다. 

이는  I/O는 임팔라의 보틀넥이 아니라는 것을 말한다. 
반면에, ORC를 사용하니 하이브의 퍼포먼스는 30%정도 향상했다.

이를 통해 임팔라는 하이브보다 훨씬 TPC-H쿼리를 효율적으로 다룬다는 것을 결론냈다.

1) 임팔라는 훨씬 효율적인 I/O 시스템을 가지고 있다. 
2) 그 자체의 데몬 프로세스에 의존하여 작업 시작 오버헤드가 없다. 
3) 임팔라의 쿼리 시행은 파이프라이닝된다. 이는 하이브의 맵리듀스가 데이터 구체화를 각 스텝에서 진행하는 것과는 반대된다. 

#### 쿼리 1
lineitem 테이블을 스캔하는데, 부등식 술어를 적용해 몇개의 열을 투영하고 수행한다. 
이 쿼리는 얼마나 I/O 레이어가 효율적으로 구성되었는지를 알려주고, 각 파일 포맷 영향을 보여준다.

- Hive-MR에서, 쿼리는 두개의 맵리듀스 잡으로 이루어져있다. 
  - 맵 단계에서, MR1은 lineitem 테이블을 스캔한다.부분적 집계를 진행한다. 리듀스 단계에서, 전체적인 집계를 한다. 
  - MR2는 MR1에서 나온 결과를 읽고 정렬한다. 
  - txt에서 ORC로 바꾸면 2.3배 속도가 빨라진다. 그를 ㅇ바축하면 더 빨라진다.
- 임팔라는 쿼리를 세 분류로 나눈다. F0/F1/F2
  - F2는 스캔하고, 예측하고, 부분적 집계를 한다. 
  - 이 결과는 그룹핑 열의 값에 기반하여, 각 노드는 셔플된다.
  - 그루핑 열의 4개의 서로 다른 값이 있기에, 4개의 노드만 데이터를 받는다. 
  - F1은 이 부분적 집계를 합치고, 각 네개의 노드에 1개의 row를 만든다. 
  - 그 다음 F0으로 스트림되어, 집계하고 정렬한다. 
  - F0 시간은 거의 밀리초 단위였고, Parquet 파일은 3.4배 정도 시간을 줄여주었다. 
- 이 모두를 통하여 봤을 때, Hive-MR보다 임팔라가 훨씬 빠른 I/O 읽기 시간을 보여주었다. 
  - 그 이유는 맵리듀스가 큰 파일을 읽을 때 오버헤드가 많기 때문이다. 
- 그러나, 우리는 스캔 과정에서 하이브와 Tez 모두가 CPU 연산이 더 크다는 것을 발견했다. 

- 임팔라는 각각의 노드에 하나의 impalad를 돌린다. 또한 각각에서 스캔과 읽기 스레드를 돌려서 더 효율적으로 프로세싱한다. 
  - 기본 있는 스토리지 시스템으로부터 바이트를 읽어 그를 스캐너에 보낸다. 
  - 이 멀티스레드 실행모델은 다른 것들보다 훨씬 효율적이다.
  - 또한 ORC 파일에 비해 Parquet파일이 훨씬 크기가 작다. 

#### 쿼리 17
- 임팔라가 하이브에 비해 2배 빠르다. 다른 쿼리에 비하여 크게 더 성능이 좋지는 않은 것이다. 
- 이는 하이브-MR의 연관 최적화가 쿼리 최적화 플랜을 짰기 때문이다.
  - 맵 단계에서, lineitem과 다른 테이블은 스캔되고, 이 예측은 적용되며 부분 집계가 l_partkey를 통해 진행된다. 
  - JOIN1과 AGG1이 하나의 스캔을 공유한다. 이는 퍼포먼스 향상을 일으키게된다. 
  - 리듀스 단계에서, 전역 집계가 일어나고, 두개의 조인이 실행된다. 
  - 여기서 CPU-bound가 일어난다.
- 임팔라는 AGG1이나 JOIN1을 분별하지못한다. 그래서, 두번 똑같은 테이블을 스캔한다.
  - 그러나, 이는 큰 영향은 아니다. 캐싱되기 때문이다.
  - 게다가, 임팔라는 데이터 구체화를 피하여 오버헤드를 막는다. 
  - 이는 쿼리를 다섯부분으로 나누어, 테이블을 먼저 스캔들을 하고, 이를 합치고, 그 결과를 모든 노드에 보낸다. 
  - 이 모든 결과는 조인되어 마지막으로 스트림되고 집계를 합친다. 
  - 임팔라는, 브로드캐스트를 통하여 조인을 한다.
  - 하나의 CPU만 사용되고 100%사용되었다. 싱글스레드를 사용하기 때문이다. 
  - 디스크는 약 10% 사용되었다. 