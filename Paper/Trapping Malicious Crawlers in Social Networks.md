## 12/16

나쁜 의도를 가지고 있는 크롤러의 문제를 살피고, 그를 막을 방법을 연구한다.

## Introduction
크롤링을 하는 것은 서치 엔진 데이터베이스를 만드는 데 있어서 중요하다. 
특별히, 그래프 샘플링에 있어서도 중요한 키가 되었다. 
이는 크롤러의 작동이 제한된 시간과 기회 내에서 가능하기 때문이다.

반대로, 크롤링은 나쁜 의도로 사용될 수도 있다. 다른 유저의 정보를 악의로 가져가는 것이나, 잘못된 프로필을 생성하는 것들이 바로 그렇다.

이를 해결하기 위하여, 트랩을 크롤링을 못하게 할 수 있다.
다른 가능한 방법은 제한적인 액세스 컨트롤을 두는 것이다. 
이 문제는 또한 약한 방법으로는, 액세스 패턴이나 트래픽을 모니터링하는 네트워크 상의 유리한 지점을 찾는 것으로 보아질 수 있다.

이 나쁜 크롤러를 막기 위하여,
시작 세트 S 노드에서 시작해, 랜덤하게 그래프를 독립적으로 돌아다닌다. 

## 관련 연구
놓여진 그래프 구조를 어떻게 조작하는지에 대한 두가지 방향이 있다. 
하나는, 한번에 노드에서 노드의 영향을 정의하는 것이고, 
하나는, 효과적으로 남아있기 위해 노드에서 노드의 영향을 허용하는 것이다. 

랜덤한 프로세스는 또 다른 확산 프로세스의 한 갈래다.
이는 근본적으로 다른 확산 모델과는 다르다. 
다른 확산 모델은 시간이 바뀌는 행동을 특징화했다면, 이것은 노드의 과정, 궤적으로 특징화된다.

게다가, 존재하는 네트워크를 조작하는 연구는 거의 없다.

## 시스템 모델과 문제 형성
### 문제 설정과 정당화
사이즈 n의 방향 없는 그래프를 생각해보자.
그 그래프는 G로 명명되며, n x n매트릭스를 가지고 있고, 노드 i와 j 사이에서 선이 있으면 Aij = 1이다.
아니면 0이다. 고민하는 문제는 그래프 G에서 어디에 트랩을 놓을지이다.

이 랜덤함의 이론적 합리성은 이 방식이 많이 쓰이기도 하고, 추천되기 때문이다.
제한된 길이 또한 다른 API나 웹 액세스 사용을 할 때 자주 보는 조건이다.

그래프를 단순히 샘플링하는 것과는 다르게, 이는 잘못 사용되는 웹 크롤링의 반대되는 면을 고려한다.

또, 트랩 개념은 여러개로 읽힐 수 있다. 
하나는 트랩은 스파이더 트랩(아예 잘못된 페이지)일수도 있고, 안티-크롤링 수단(캡챠같은)을 사용하는 것일 수도 있다.
또 다른 하나는 제한된 액세스 콘트롤이 있는 유저 페이지일수도 있다. 
넓은 범위로는, 이러한 접근을 파악하는 네트워크 포인트가 될 수도 있다.

요약하자면, 실질적으로 이 크롤링을 막는 트랩 개념은 넓은 범위에서 사용되며, 그 트랩이 어디에 설치되어야하는지에 대한 문제를 살펴볼 것이다.

### 모델 설명
K라는 나쁜 크롤러가 있다. 랜덤하게 L이라는 개수다.
L은 한정된 길이고, 이 k는 노드 위를 돌아다닌다. k가 돌아다니는 노드를 희생자라고 부른다. 
우리는 k가 그 이웃 노드로 옮겨갈 때, S가 아닌 곳으로 옮겨간다고 가정한다. 여기서 S는 다른 노드와 연결될 필요가 없는 노드다.
그러므로, k는 원래 그래프 G가 아니라, 실제로는 좀 변형된 G를 돌아다닌다고 볼 수 있다. 

u라는 노드에서 v라는 노드로 옮겨가는 것 (변형된 G 위에서)을 Puv = 1/du라고 한다. du는 u의 degree다. 

V를 피해받은 노드의 개수라고 하자. 수학적으로 취급이 쉽기 위해서, 이를 피해자의 총합값이라고 생각한다.
또한, 이 제안된 알고리즘을 확장된 시뮬레이션으로 평가한다.

k 크롤러를 좀 고쳐서, k에 의해 희생된 S노드 가능성을 평가해본다.
Xk(t)는 t 타임의 위치, Tk(i)는 처음으로 해당 노드를 방문했을 때의 시간이다.

기본형 1. S에서 나온 랜덤 노드에서 시작해 node i 가 크롤러 k에게 희생되는 가능성 언급


### 문제 언급
크롤러를 오도가도 못하게 막는 W라는 엣지를 둔다. W가 있고 없고의 V 수 차이 확인을 위하여, V와 V(W)값을 둔다.

## 시뮬레이션 결과
실제 세상에서 이 알고리즘 실험을 한다.
자가반복을을 막기위해, 각 소셜 네트웤 그래프를 전처리한다. 
그리고 가장 연결이 많은 컴포넌트를 사용한다.

### 시뮬레이션 설정
1. K=100인 경우, 동시에 크롤러가 시작해 끝날때까지 진행한다.
2. 각기 다른 시간에 시작한다. 시간을 신중하게 나누고, 각각의 시간대에 크롤러를 집어넣는다. 그리고 그 시간대는 크롤러의 생애주기다. 

S사이즈가 주어진 두개의 다른 케이스들도 고려한다.
하나는 모든 S내의 노드들이 클러스터를 이루는 것이다. 이는 반드시 노드들이 직접적으로 연결되어있음을 말하지는 않는다.
다른 하나는 분산형이다. 

각각의 데이터 포인트는 독립적 시뮬레이션 10^4개를 거쳐 평균을 낸 것이다. 

특정 조건이 주어졌을 때, 우리는 표준 중심성 수단을 고려한다.

### 시뮬레이션 결과
희생자 수에 있어서 압도적으로 적은 결과를 보였다. 
예산을 더 늘리면 더욱더 좋은 퍼포먼스 향상이 있었다.

우리는 또한 시간이 지남에 따라 희생자 수가 어떻게 바뀌는지 보았다. 
우리 알고리즘은 다른 것에 비해 계속 일정했다. 

두번째 케이스를 시뮬레이션해봤을 때,
유의미한 중심성에 따른 희생자의 변화는 없는 것으로 보여졌다. 
노드들이 분산돼있을 때도, 알고리즘은 압도적으로 잘 행동했다.


## 결론
우리는 나쁜 크롤러를 상대로 어디에 트랩을 놓을지에 대한 문제를 연구했다. 
이는 monotone submodular maximization문제라는 것을 깨달았고, 그래서 (1-1/e) 예측알고리즘을 구상했다.
게다가, 중요한 V(w)요소를 두어, 전산적으로 효율적이고 스케일러블하게 했다.

시뮬레이션으로 이러한 알고리즘이 압도적인 좋은 결과를 냈음도 증명했다.